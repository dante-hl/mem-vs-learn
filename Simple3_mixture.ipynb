{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06258bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/induction/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from random import choices, Random, sample, random\n",
    "from random import seed as randomseed\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # change 'cuda' to something else...?\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO put num_symbols inside of the main training loop and parametrize these functions with it\n",
    "num_symbols = 3\n",
    "\n",
    "# generator for getting uniformly random transition matrices\n",
    "# dirichlet_markov_ensemble.sample([n]) returns a tensor of n uniformly random num_symbols x num_symbols transition matrices\n",
    "dirichlet_markov_ensemble = torch.distributions.dirichlet.Dirichlet(torch.ones((num_symbols,num_symbols), device = device))\n",
    "\n",
    "\n",
    "# faster method of finding stationary distribution of a markov chain, technically an approximation, and I haven't tested for many different number of symbols. Robust method commented out below\n",
    "def stationary_distribution(prob):\n",
    "    return torch.linalg.matrix_power(prob, power).mean(axis=1)\n",
    "# only for the stationary_distribution function, chosen a bit arbitrarily\n",
    "power = int(2**np.ceil(np.log2(num_symbols*3)))\n",
    "# def stationary_distribution(prob):\n",
    "  \n",
    "#     evals, evecs = torch.linalg.eig(prob.transpose(1,2))\n",
    "#     # evec1 = evecs[torch.isclose(evals, torch.ones(1, dtype=torch.complex64)),:]\n",
    "#     #Since np.isclose will return an array, we've indexed with an array\n",
    "#     #so we still have our 2nd axis.  Get rid of it, since it's only size 1.\n",
    "#     #  evec1 = evec1[...,0].real\n",
    "#     evec1 = evecs[range(len(evecs)),:, torch.argmax(evals.real, dim=1).squeeze()]\n",
    "#     stationary = (evec1.T / evec1.sum(axis=1)).T\n",
    "#     # print(stationary)\n",
    "\n",
    "#     #eigs finds complex eigenvalues and eigenvectors, so you'll want the real part.\n",
    "#     return stationary.real\n",
    "\n",
    "# samples autoregressively from markov chains for length symbols. first symbol is sampled from stationary distribution. if transition_matrices is an integer, then that many uniformly random ones are sampled\n",
    "def data_gen(length, transition_matrices=None):\n",
    "    if transition_matrices is None:\n",
    "        transition_matrices = dirichlet_markov_ensemble.sample([64])\n",
    "    elif type(transition_matrices) is int:\n",
    "        transition_matrices = dirichlet_markov_ensemble.sample([transition_matrices])\n",
    "    \n",
    "    stat_dists = stationary_distribution(transition_matrices)\n",
    "    # thresholds = transition_matrices.cumsum(axis = 2).tolist()\n",
    "    output = torch.zeros(len(transition_matrices), length, dtype=int, device = device)\n",
    "    output[:, 0] = torch.multinomial(stat_dists, 1).squeeze()\n",
    "    cons = torch.arange(len(transition_matrices), device = device) * 3\n",
    "    for ind in range(1, length):\n",
    "        temp = transition_matrices.view(num_symbols*len(transition_matrices), num_symbols)[cons + output[:,ind-1]]\n",
    "        output[:,ind] = torch.multinomial(temp,1).squeeze()\n",
    "    return output.to(device)\n",
    "\n",
    "def mixture_transition_matrices(num_matrices, fixed):\n",
    "    return torch.stack([dirichlet_markov_ensemble.sample() if random()>0.5 else fixed for _ in range(num_matrices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to calculate how close the model is to various strategies, these will help:\n",
    "def unigram_predict_next(sample):\n",
    "    counts = torch.bincount(sample, minlength=32)\n",
    "    return counts/counts.sum()\n",
    "\n",
    "@torch.no_grad()\n",
    "def ngram(inp, num_symbols=num_symbols, n=2):    \n",
    "    ngrams = zip(*[inp[i:] for i in range(n)])\n",
    "    candidate = torch.ones(num_symbols, dtype=torch.float)\n",
    "    check = tuple(inp[-n+1:])\n",
    "    for i in ngrams:\n",
    "        if i[:-1] == check or n == 1:\n",
    "            candidate[i[-1]]+=1\n",
    "    candidate = F.normalize(candidate, p=1, dim=0)\n",
    "    return candidate\n",
    "\n",
    "\n",
    "def model_alg_kl_div(model, test_batch, alg):\n",
    "    preds = model(test_batch)[:,-1]\n",
    "    return F.kl_div(F.log_softmax(preds, dim=1), torch.stack([alg(x) for x in test_batch]).to(preds.device), reduction=\"sum\")/ len(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, drop, hid_dim = 512, n_head = 8, n_layer = 6, max_position = 50):\n",
    "        super().__init__()\n",
    "        config = GPT2Config(vocab_size = input_dim, n_embd= hid_dim, n_layer = n_layer, n_head = n_head, \\\n",
    "                            activation_function= 'gelu', n_positions= max_position, \\\n",
    "                             resid_pdrop = drop, embd_pdrop = drop, attn_pdrop = drop, use_cache=False, n_inner = hid_dim*4)\n",
    "        \n",
    "        self.GPT2= GPT2Model(config)\n",
    "        self.lin2= nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.GPT2(input_ids= x, attention_mask = torch.ones_like(x)).last_hidden_state\n",
    "        last= self.lin2(hidden)\n",
    "\n",
    "        return last\n",
    "    \n",
    "def test(model, loss_fn, test_batch):\n",
    "    x_t = test_batch\n",
    "    test_starting_at = 0\n",
    "    preds_valid = model(x_t[:, test_starting_at:-1])\n",
    "    return loss_fn(preds_valid.mT, x_t[:, test_starting_at+1:]).cpu()\n",
    "\n",
    "def test_true(model, loss_fn, test_batch, test_matrices, device):\n",
    "    x_t = test_batch \n",
    "    test_starting_at = 0\n",
    "    preds_valid = model(x_t[:, test_starting_at:-1])\n",
    "    labels = test_matrices.to(device)[torch.arange(test_matrices.size(0)).unsqueeze(1), x_t[:, test_starting_at:-1]]\n",
    "    return F.kl_div(F.log_softmax(preds_valid, dim=-1), labels, reduction=\"none\").mT.mean(dim=[0,1]).cpu()\n",
    "\n",
    "\n",
    "# training code! \n",
    "# length is the length of the training sequences\n",
    "# every is how often to test the model\n",
    "# fixed is the fixed transition matrix\n",
    "# data_gen_batches is how many batches to compute at once. bigger is better effeciency (None sets it to do all) but uses up more memory\n",
    "# TODO add ability to train on different distributions (mixture, just uniform, just fixed, etc)\n",
    "def trainer(model, iters, opt, loss_fn, length, every= 2000, device= device, fixed = None, batchsize=64, data_gen_batches = None):\n",
    "    model.train()\n",
    "    test_loss = []\n",
    "    train_loss = []\n",
    "    if data_gen_batches is None:\n",
    "        data_gen_batches = iters + 1\n",
    "    fixed_batch = torch.stack([fixed]*batchsize)\n",
    "    fixed_test_batch = data_gen(length, fixed_batch).to(device)\n",
    "    fixed_test_loss = []\n",
    "    test_batch_matrices = dirichlet_markov_ensemble.sample([batchsize])\n",
    "    test_batch = data_gen(length, test_batch_matrices).to(device)\n",
    "    data = data_gen(length, mixture_transition_matrices(batchsize*data_gen_batches, fixed))\n",
    "    data_index = 0\n",
    "    #TODO have tqdm show the train loss\n",
    "    for i in tqdm(range(iters+1), ncols = 100, desc = \"Progress\", position = 0, leave = True):\n",
    "\n",
    "        if i%every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_loss.append(test(model, loss_fn, test_batch))\n",
    "                test_loss.append(test_true(model, loss_fn, test_batch, test_batch_matrices, device))\n",
    "                fixed_test_loss.append(test_true(model, loss_fn, fixed_test_batch, fixed_batch, device))\n",
    "            model.train()\n",
    "        \n",
    "        x_t = data[data_index:data_index + batchsize]\n",
    "        data_index+= batchsize\n",
    "        if data_index >= batchsize*data_gen_batches:\n",
    "            data = data_gen(length, mixture_transition_matrices(batchsize*data_gen_batches, fixed))\n",
    "            data_index = 0\n",
    "        preds= model(x_t[:,:-1])\n",
    "        loss= loss_fn(preds.mT, x_t[:, 1:])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    print(\"final loss= %f\"%(loss.detach().cpu().numpy()))\n",
    "    \n",
    "    return torch.stack(test_loss), torch.stack(fixed_test_loss), train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main cell to run the training!\n",
    "seed = 4\n",
    "torch.manual_seed(seed)\n",
    "randomseed(seed)\n",
    "np.random.seed(seed)\n",
    "fixed = dirichlet_markov_ensemble.sample()\n",
    "length = 30\n",
    "model = gpt(3, 3, drop = 0, hid_dim = 16, n_head = 1, n_layer = 2, max_position = length - 1)\n",
    "model.to(device= device)\n",
    "model.train()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr= 1e-3, weight_decay= 0)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "print(fixed)\n",
    "temp = trainer(model, 4000, opt, loss_fn, length, every = 4, device= device, fixed = fixed)\n",
    "accs, fixed_accs, train_loss = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62651df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs.mean(axis=1), label = \"Test loss\")\n",
    "plt.plot(fixed_accs.mean(axis=1), label = \"Test loss on fixed transition matrix\")\n",
    "plt.xlabel(\"Amount of training\")\n",
    "plt.ylabel(\"Test Loss at last token\")\n",
    "plt.title(\"Training Transformer on MC-ICL\\n(3 symbols, length 101 training)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "pos = -1\n",
    "plt.plot(accs[-1], label = \"Test loss\")\n",
    "plt.plot(fixed_accs[-1], label = \"Test loss on fixed transition matrix\")\n",
    "plt.xlabel(\"Tokens of Context\")\n",
    "plt.ylabel(\"Test Loss at last token\")\n",
    "plt.title(\"Training Transformer on Mixture\\n(3 symbols, length 101 training)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(train_loss)\n",
    "plt.show()\n",
    "#plot loss at different tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
